{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import json\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./previews'):\n",
    "    os.makedirs('./previews')\n",
    "if not os.path.exists('./checkpoints'):\n",
    "    os.makedirs('./checkpoints')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 64\n",
    "n_words = 20 # 五言絕句\n",
    "n_class = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poet_filter(x):\n",
    "    return len(''.join(x['paragraphs']))==20 and len(''.join(x['strains']))==20 # 五言絕句，不包含逗號句號分號\n",
    "\n",
    "def poet_preprocess(x): # 去除標點符號\n",
    "    x['paragraphs'] = ''.join(filter(lambda x: x!='，' and x!='。' and x!='；' ,list(''.join(x['paragraphs']))))\n",
    "    x['strains'] = ''.join(filter(lambda x: x!='，' and x!='。' and x!='；' , list(''.join(x['strains']))))\n",
    "    return x\n",
    "\n",
    "def poet_data_reader(search='./chinese-poetry-master/json/poet.song.*.json', filters=lambda x: True):\n",
    "    file_list = glob.glob(search)\n",
    "    data = []\n",
    "    for fname in file_list:\n",
    "        with open(fname, 'r') as fp:\n",
    "            data += filter(poet_filter, map(poet_preprocess , json.loads(fp.read())))\n",
    "    return data\n",
    "def gen_dict(poets):\n",
    "    char_set = dict()\n",
    "    char_set_inv = dict()\n",
    "    for poet in poets:\n",
    "        context = list(poet['paragraphs'])\n",
    "        for c in filter(lambda x: x not in char_set, context):\n",
    "            l = len(char_set)\n",
    "            char_set[c] = l\n",
    "            char_set_inv[l] = c\n",
    "    return char_set, char_set_inv\n",
    "def encode_context(context, charset):\n",
    "    def f(x):\n",
    "        return charset[x] if x in charset else 0\n",
    "    return list(map(f, list(context)))\n",
    "def one_hot(x, n_class):\n",
    "    ohe = np.zeros((len(x), n_class), dtype=np.uint8)\n",
    "    ohe[np.arange(len(x)), x] = 1\n",
    "    return ohe\n",
    "def str2ohe(x, charset):\n",
    "    return one_hot(encode_context(x, charset), len(charset))\n",
    "def ohe2str(x, charset_inv):\n",
    "    x = np.argmax(x,axis=-1)\n",
    "    return ''.join(list(map(lambda a: charset_inv[a], list(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = poet_data_reader(search='./chinese-poetry-master/json/poet.song.*.json', filters=poet_filter)\n",
    "charset, charset_inv = gen_dict(raw_data)\n",
    "strainset = {'平': 0, '仄': 1}\n",
    "strainset_inv = {0: '平', 1: '仄'}\n",
    "with open('./charset.json', 'w') as fp:\n",
    "    fp.write(json.dumps(charset))\n",
    "with open('./strainset.json', 'w') as fp:\n",
    "    fp.write(json.dumps(strainset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13260, 5798, 20])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ohe = torch.from_numpy(np.asarray(list(map(lambda x: str2ohe(x['paragraphs'], charset), raw_data)), dtype=np.float32).transpose(0,2,1))\n",
    "data_ohe = data_ohe * 2 - 1 # [0, 1] -> [-1, +1]\n",
    "data_ohe.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13260, 2, 20])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_ohe = torch.from_numpy(np.asarray(list(map(lambda x: str2ohe(x['strains'], strainset), raw_data)), dtype=np.float32).transpose(0,2,1))\n",
    "label_ohe = label_ohe * 2 - 1 # [0, 1] -> [-1, +1]\n",
    "label_ohe.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_class = data_ohe.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "poet_dataset = torch.utils.data.TensorDataset(data_ohe, label_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(\n",
    "        poet_dataset,\n",
    "        batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "def inf_data_gen():\n",
    "    while True:\n",
    "        for data, label in data_loader:\n",
    "            yield data, label\n",
    "gen = inf_data_gen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0, 0.001)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0, 0.001)\n",
    "\n",
    "def wgan_div_gp(real, d_real, device, p):\n",
    "    ones_real = torch.ones_like(d_real, device=device, requires_grad=False)\n",
    "    gradients_real = torch.autograd.grad(\n",
    "            outputs=d_real,\n",
    "            inputs=real,\n",
    "            grad_outputs=ones_real,\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True,\n",
    "        )[0]\n",
    "    return gradients_real.view(gradients_real.size(0),-1).pow(2).sum(1)**(p/2) \n",
    "\n",
    "class C(nn.Module):\n",
    "    def __init__(self, n_words=20, n_class=5000):\n",
    "        super(C, self).__init__()\n",
    "        self.n_words = n_words\n",
    "        self.n_class = n_class\n",
    "        self.net = nn.Sequential(*[\n",
    "            nn.Conv1d(self.n_class+2, 300, kernel_size=1, padding=0, bias=False), # embedding\n",
    "            nn.Conv1d(300, 64, kernel_size=3, stride=1, padding=1, bias=False), # 10\n",
    "            nn.InstanceNorm1d(64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv1d(64, 128, kernel_size=5, stride=2, padding=2, bias=False), # 10\n",
    "            nn.InstanceNorm1d(128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv1d(128, 256, kernel_size=3, stride=1, padding=1, bias=False), # 10\n",
    "            nn.InstanceNorm1d(256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv1d(256, 256, kernel_size=3, stride=1, padding=1, bias=False), # 10\n",
    "            nn.InstanceNorm1d(256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv1d(256, 512, kernel_size=3, stride=2, padding=1, bias=False), # 5\n",
    "            nn.InstanceNorm1d(512),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv1d(512, 1, kernel_size=n_words//4, padding=0, bias=False)\n",
    "        ])\n",
    "        self.net.apply(weights_init)\n",
    "        \n",
    "    def forward(self, x, label):\n",
    "        x = torch.cat((x, label), 1)\n",
    "        x = self.net(x)\n",
    "        x = x.view(x.size(0),1)\n",
    "        return x\n",
    "\n",
    "class G(nn.Module):\n",
    "    def __init__(self, n_words=20, n_class=5000):\n",
    "        super(G, self).__init__()\n",
    "        self.n_words = n_words\n",
    "        self.n_class = n_class\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(2, 64, kernel_size=3, stride=1, padding=1, bias=False) # 20\n",
    "        self.norm1 = nn.InstanceNorm1d(64)\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=5, stride=2, padding=2, bias=False) # 10\n",
    "        self.norm2 = nn.InstanceNorm1d(128)\n",
    "        self.conv3 = nn.Conv1d(128, 256, kernel_size=3, stride=1, padding=1, bias=False) # 10\n",
    "        self.norm3 = nn.InstanceNorm1d(256)\n",
    "        self.conv4 = nn.Conv1d(256, 512, kernel_size=3, stride=2, padding=1, bias=False) # 5\n",
    "        self.norm4 = nn.InstanceNorm1d(512)\n",
    "        self.upconv5 = nn.ConvTranspose1d(512, 256, kernel_size=4, stride=2, padding=1, bias=False) # 10\n",
    "        self.norm5 = nn.InstanceNorm1d(256+256)\n",
    "        self.upconv6 = nn.ConvTranspose1d(256+256, 300, kernel_size=4, stride=2, padding=1, bias=False) # 20\n",
    "        self.norm6 = nn.InstanceNorm1d(300+64)\n",
    "        self.conv7 = nn.Conv1d(300+64, self.n_class, kernel_size=1, stride=1, padding=0, bias=False) # 20\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = F.leaky_relu(x, 0.1)\n",
    "        x = F.dropout(x, 0.5)\n",
    "        s1 = x\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = F.leaky_relu(x, 0.1)\n",
    "        x = F.dropout(x, 0.5)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.norm3(x)\n",
    "        x = F.leaky_relu(x, 0.1) # 10\n",
    "        s2 = x\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = self.norm4(x)\n",
    "        x = F.leaky_relu(x, 0.1) # 5\n",
    "        \n",
    "        x = self.upconv5(x)\n",
    "        x = torch.cat((x, s2), 1)\n",
    "        x = self.norm5(x)\n",
    "        x = F.leaky_relu(x, 0.1) # 10\n",
    "        \n",
    "        x = self.upconv6(x)\n",
    "        x = torch.cat((x, s1), 1)\n",
    "        x = self.norm6(x)\n",
    "        x = F.leaky_relu(x, 0.1) # 20\n",
    "        \n",
    "        x = self.conv7(x) # 20\n",
    "        x = torch.tanh(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 3 # debug!!!\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "G_net = G(n_words, n_class).to(device).apply(weights_init)\n",
    "C_net = C(n_words, n_class).to(device)\n",
    "opt_C = optim.Adam(C_net.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "opt_G = optim.Adam(G_net.parameters(), lr=0.0002, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a6b2787b7ff49ec85d77622f878d626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500/50000] G: -17.9354, D:18.1902 -- elapsed_G: 0.0365s -- elapsed_D: 0.8171s\n",
      "[1000/50000] G: -15.7259, D:12.1920 -- elapsed_G: 0.0370s -- elapsed_D: 0.8025s\n",
      "[1500/50000] G: -10.9415, D:-0.4948 -- elapsed_G: 0.0373s -- elapsed_D: 0.8086s\n",
      "[2000/50000] G: -6.4888, D:-11.6378 -- elapsed_G: 0.0369s -- elapsed_D: 0.8043s\n",
      "[2500/50000] G: 23.1075, D:-80.0368 -- elapsed_G: 0.0368s -- elapsed_D: 0.8073s\n",
      "[3000/50000] G: -8.4262, D:-2.9058 -- elapsed_G: 0.0369s -- elapsed_D: 0.8061s\n",
      "[3500/50000] G: -10.8187, D:-0.5899 -- elapsed_G: 0.0369s -- elapsed_D: 0.8000s\n",
      "[4000/50000] G: 2.7920, D:-0.1194 -- elapsed_G: 0.0369s -- elapsed_D: 0.8052s\n",
      "[4500/50000] G: -12.5451, D:-0.0528 -- elapsed_G: 0.0363s -- elapsed_D: 0.8140s\n",
      "[5000/50000] G: -0.9952, D:0.1048 -- elapsed_G: 0.0369s -- elapsed_D: 0.8009s\n",
      "[5500/50000] G: -2.1822, D:-0.0239 -- elapsed_G: 0.0370s -- elapsed_D: 0.7965s\n",
      "[6000/50000] G: -2.0761, D:-0.0904 -- elapsed_G: 0.0369s -- elapsed_D: 0.8062s\n",
      "[6500/50000] G: -2.1621, D:-0.0447 -- elapsed_G: 0.0408s -- elapsed_D: 0.8521s\n",
      "[7000/50000] G: -2.2042, D:0.0008 -- elapsed_G: 0.0376s -- elapsed_D: 0.8065s\n",
      "[7500/50000] G: -2.0566, D:-0.0045 -- elapsed_G: 0.0381s -- elapsed_D: 0.8237s\n",
      "[8000/50000] G: 16.1688, D:-0.0128 -- elapsed_G: 0.0383s -- elapsed_D: 0.8381s\n",
      "[8500/50000] G: -19.3130, D:-0.0043 -- elapsed_G: 0.0370s -- elapsed_D: 0.8074s\n",
      "[9000/50000] G: 3.4553, D:-0.0101 -- elapsed_G: 0.0377s -- elapsed_D: 0.8057s\n",
      "[9500/50000] G: -10.4045, D:-0.0040 -- elapsed_G: 0.0375s -- elapsed_D: 0.8055s\n",
      "[10000/50000] G: -7.8887, D:0.0223 -- elapsed_G: 0.0373s -- elapsed_D: 0.8061s\n",
      "[10500/50000] G: -6.6659, D:-0.0010 -- elapsed_G: 0.0372s -- elapsed_D: 0.8077s\n",
      "[11000/50000] G: -6.8999, D:-0.0028 -- elapsed_G: 0.0375s -- elapsed_D: 0.8049s\n",
      "[11500/50000] G: -6.5828, D:-0.0011 -- elapsed_G: 0.0373s -- elapsed_D: 0.8048s\n",
      "[12000/50000] G: -3.5328, D:-0.0098 -- elapsed_G: 0.0375s -- elapsed_D: 0.8081s\n",
      "[12500/50000] G: -4.2655, D:-0.0152 -- elapsed_G: 0.0376s -- elapsed_D: 0.8056s\n",
      "[13000/50000] G: -5.7908, D:-0.0022 -- elapsed_G: 0.0145s -- elapsed_D: 0.8067s\n",
      "[13500/50000] G: 2.8306, D:-0.0052 -- elapsed_G: 0.0373s -- elapsed_D: 0.8071s\n",
      "[14000/50000] G: 6.4757, D:-0.0458 -- elapsed_G: 0.0374s -- elapsed_D: 0.8063s\n",
      "[14500/50000] G: 10.4810, D:-0.0394 -- elapsed_G: 0.0372s -- elapsed_D: 0.8060s\n",
      "[15000/50000] G: 6.3882, D:-0.0007 -- elapsed_G: 0.0375s -- elapsed_D: 0.8053s\n",
      "[15500/50000] G: 9.7786, D:-0.0929 -- elapsed_G: 0.0374s -- elapsed_D: 0.8074s\n",
      "[16000/50000] G: 7.4468, D:-0.2047 -- elapsed_G: 0.0368s -- elapsed_D: 0.8016s\n",
      "[16500/50000] G: 2.5330, D:-0.3303 -- elapsed_G: 0.0375s -- elapsed_D: 0.8069s\n",
      "[17000/50000] G: -0.1667, D:-0.3809 -- elapsed_G: 0.0374s -- elapsed_D: 0.8079s\n",
      "[17500/50000] G: 0.8797, D:-0.4039 -- elapsed_G: 0.0369s -- elapsed_D: 0.8085s\n",
      "[18000/50000] G: 1.7257, D:-0.4342 -- elapsed_G: 0.0370s -- elapsed_D: 0.8059s\n",
      "[18500/50000] G: 2.7090, D:-0.1161 -- elapsed_G: 0.0377s -- elapsed_D: 0.8078s\n",
      "[19000/50000] G: 3.2354, D:-0.3313 -- elapsed_G: 0.0373s -- elapsed_D: 0.8055s\n",
      "[19500/50000] G: 1.6340, D:-0.3585 -- elapsed_G: 0.0377s -- elapsed_D: 0.8031s\n",
      "[20000/50000] G: -0.2191, D:-0.3441 -- elapsed_G: 0.0368s -- elapsed_D: 0.8100s\n",
      "[20500/50000] G: -0.6837, D:-0.3552 -- elapsed_G: 0.0373s -- elapsed_D: 0.8047s\n",
      "[21000/50000] G: -1.3258, D:-0.3207 -- elapsed_G: 0.0369s -- elapsed_D: 0.8094s\n",
      "[21500/50000] G: -0.6600, D:-0.2914 -- elapsed_G: 0.0372s -- elapsed_D: 0.8065s\n",
      "[22000/50000] G: -1.7409, D:-0.3159 -- elapsed_G: 0.0376s -- elapsed_D: 0.8056s\n",
      "[22500/50000] G: -0.4219, D:-0.2764 -- elapsed_G: 0.0372s -- elapsed_D: 0.7986s\n",
      "[23000/50000] G: 0.3129, D:-0.3547 -- elapsed_G: 0.0374s -- elapsed_D: 0.8061s\n",
      "[23500/50000] G: -0.1599, D:-0.3015 -- elapsed_G: 0.0374s -- elapsed_D: 0.8064s\n",
      "[24000/50000] G: -0.0793, D:-0.1919 -- elapsed_G: 0.0371s -- elapsed_D: 0.8049s\n",
      "[24500/50000] G: -0.2598, D:-0.1906 -- elapsed_G: 0.0376s -- elapsed_D: 0.8051s\n",
      "[25000/50000] G: -1.1584, D:0.0007 -- elapsed_G: 0.0370s -- elapsed_D: 0.8051s\n",
      "[25500/50000] G: -1.0158, D:-0.2342 -- elapsed_G: 0.0373s -- elapsed_D: 0.8060s\n",
      "[26000/50000] G: -0.2360, D:-0.1094 -- elapsed_G: 0.0146s -- elapsed_D: 0.8049s\n",
      "[26500/50000] G: 0.7578, D:-0.1542 -- elapsed_G: 0.0371s -- elapsed_D: 0.8059s\n",
      "[27000/50000] G: 0.0659, D:-0.2327 -- elapsed_G: 0.0374s -- elapsed_D: 0.8052s\n",
      "[27500/50000] G: 0.5818, D:-0.1393 -- elapsed_G: 0.0372s -- elapsed_D: 0.8082s\n",
      "[28000/50000] G: 0.8890, D:-0.2125 -- elapsed_G: 0.0372s -- elapsed_D: 0.8067s\n",
      "[28500/50000] G: 1.3095, D:-0.2367 -- elapsed_G: 0.0374s -- elapsed_D: 0.8085s\n",
      "[29000/50000] G: 0.9568, D:-0.1962 -- elapsed_G: 0.0364s -- elapsed_D: 0.8063s\n",
      "[29500/50000] G: 1.0738, D:-0.2787 -- elapsed_G: 0.0377s -- elapsed_D: 0.8058s\n",
      "[30000/50000] G: 1.0369, D:-0.1278 -- elapsed_G: 0.0371s -- elapsed_D: 0.8040s\n",
      "[30500/50000] G: 2.3092, D:-0.2560 -- elapsed_G: 0.0372s -- elapsed_D: 0.8339s\n",
      "[31000/50000] G: 3.1945, D:-0.2170 -- elapsed_G: 0.0377s -- elapsed_D: 0.8238s\n",
      "[31500/50000] G: 2.2453, D:-0.2474 -- elapsed_G: 0.0375s -- elapsed_D: 0.8283s\n",
      "[32000/50000] G: 3.0831, D:-0.2608 -- elapsed_G: 0.0420s -- elapsed_D: 0.8553s\n",
      "[32500/50000] G: 2.4820, D:-0.2407 -- elapsed_G: 0.0374s -- elapsed_D: 0.8046s\n",
      "[33000/50000] G: 2.9348, D:-0.2657 -- elapsed_G: 0.0379s -- elapsed_D: 0.8067s\n",
      "[33500/50000] G: 2.9087, D:-0.1866 -- elapsed_G: 0.0389s -- elapsed_D: 0.8254s\n",
      "[34000/50000] G: 3.5708, D:-0.2211 -- elapsed_G: 0.0383s -- elapsed_D: 0.8272s\n",
      "[34500/50000] G: 2.6284, D:-0.1799 -- elapsed_G: 0.0386s -- elapsed_D: 0.8221s\n",
      "[35000/50000] G: 3.5894, D:-0.2053 -- elapsed_G: 0.0391s -- elapsed_D: 0.8349s\n",
      "[35500/50000] G: 3.2444, D:-0.1369 -- elapsed_G: 0.0391s -- elapsed_D: 0.8310s\n",
      "[36000/50000] G: 4.0074, D:-0.1350 -- elapsed_G: 0.0376s -- elapsed_D: 0.8045s\n",
      "[36500/50000] G: 6.2352, D:-0.1222 -- elapsed_G: 0.0386s -- elapsed_D: 0.8262s\n",
      "[37000/50000] G: 5.4917, D:-0.2664 -- elapsed_G: 0.0375s -- elapsed_D: 0.8059s\n",
      "[37500/50000] G: 5.1534, D:-0.1760 -- elapsed_G: 0.0372s -- elapsed_D: 0.8057s\n",
      "[38000/50000] G: 5.1260, D:-0.1256 -- elapsed_G: 0.0376s -- elapsed_D: 0.8068s\n",
      "[38500/50000] G: 5.0611, D:-0.0744 -- elapsed_G: 0.0370s -- elapsed_D: 0.8074s\n",
      "[39000/50000] G: 5.1610, D:-0.1605 -- elapsed_G: 0.0149s -- elapsed_D: 0.8063s\n",
      "[39500/50000] G: 5.0846, D:-0.1097 -- elapsed_G: 0.0374s -- elapsed_D: 0.8052s\n",
      "[40000/50000] G: 4.0549, D:-0.1791 -- elapsed_G: 0.0376s -- elapsed_D: 0.8047s\n",
      "[40500/50000] G: 4.3567, D:-0.1596 -- elapsed_G: 0.0370s -- elapsed_D: 0.8059s\n",
      "[41000/50000] G: 4.4174, D:-0.1556 -- elapsed_G: 0.0375s -- elapsed_D: 0.8049s\n",
      "[41500/50000] G: 3.7472, D:-0.1960 -- elapsed_G: 0.0375s -- elapsed_D: 0.8068s\n",
      "[42000/50000] G: 3.7042, D:-0.2195 -- elapsed_G: 0.0373s -- elapsed_D: 0.8064s\n",
      "[42500/50000] G: 4.0095, D:-0.0993 -- elapsed_G: 0.0376s -- elapsed_D: 0.8037s\n",
      "[43000/50000] G: 4.3762, D:-0.1466 -- elapsed_G: 0.0378s -- elapsed_D: 0.8065s\n",
      "[43500/50000] G: 4.3443, D:-0.1685 -- elapsed_G: 0.0368s -- elapsed_D: 0.8095s\n",
      "[44000/50000] G: 4.1460, D:-0.1427 -- elapsed_G: 0.0375s -- elapsed_D: 0.8045s\n",
      "[44500/50000] G: 4.7129, D:-0.2118 -- elapsed_G: 0.0369s -- elapsed_D: 0.8082s\n",
      "[45000/50000] G: 4.7402, D:-0.1948 -- elapsed_G: 0.0374s -- elapsed_D: 0.8089s\n",
      "[45500/50000] G: 5.3655, D:-0.2242 -- elapsed_G: 0.0375s -- elapsed_D: 0.8068s\n",
      "[46000/50000] G: 7.6308, D:-0.8891 -- elapsed_G: 0.0374s -- elapsed_D: 0.8052s\n",
      "[46500/50000] G: 8.3949, D:-0.1948 -- elapsed_G: 0.0369s -- elapsed_D: 0.8068s\n",
      "[47000/50000] G: 8.5075, D:-0.1743 -- elapsed_G: 0.0376s -- elapsed_D: 0.8055s\n",
      "[47500/50000] G: 8.8654, D:-0.2379 -- elapsed_G: 0.0371s -- elapsed_D: 0.8074s\n",
      "[48000/50000] G: 9.1061, D:0.0139 -- elapsed_G: 0.0377s -- elapsed_D: 0.8063s\n",
      "[48500/50000] G: 8.4093, D:0.0395 -- elapsed_G: 0.0370s -- elapsed_D: 0.8014s\n",
      "[49000/50000] G: 9.3697, D:-0.2086 -- elapsed_G: 0.0374s -- elapsed_D: 0.8052s\n",
      "[49500/50000] G: 9.4965, D:-0.2205 -- elapsed_G: 0.0383s -- elapsed_D: 0.8060s\n",
      "[50000/50000] G: 8.4622, D:-0.2287 -- elapsed_G: 0.0371s -- elapsed_D: 0.8080s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "iterations = 50000\n",
    "preview_iter = 500\n",
    "preview_n = 8\n",
    "d_iter = 5\n",
    "std = 1.0\n",
    "lambda_1 , lambda_2 = 10 , 0.2\n",
    "M = 0.05\n",
    "k, p = 2, 6\n",
    "beta = 0.001\n",
    "\n",
    "for ite in tqdm_notebook(range(1, iterations+1)):\n",
    "    start_train_ts = time.time()\n",
    "    # train D:\n",
    "    G_net.train() # enable dropout\n",
    "    C_net.train()\n",
    "    d_loss_mean = 0.0\n",
    "    g_loss_mean = 0.0\n",
    "    for _ in range(d_iter):\n",
    "        opt_C.zero_grad()\n",
    "        real, label = next(gen)\n",
    "        real = real.to(device).requires_grad_(True)\n",
    "        label = label.to(device).requires_grad_(True)\n",
    "        with torch.no_grad():\n",
    "            fake   = G_net(label).detach() # not to touch G_net\n",
    "        fake.requires_grad_(True)\n",
    "        d_real = C_net(real,label)\n",
    "        d_fake = C_net(fake,label)\n",
    "        d_loss_real = d_real.mean()\n",
    "        d_loss_real.backward(retain_graph=True)\n",
    "        d_loss_fake = -d_fake.mean()\n",
    "        d_loss_fake.backward(retain_graph=True)\n",
    "        d_real_gp = wgan_div_gp(real, d_real, device, p)\n",
    "        d_fake_gp = wgan_div_gp(fake, d_fake, device, p)\n",
    "        d_gp_loss = (d_real_gp+d_fake_gp).mean() * k / 2\n",
    "        d_gp_loss.backward(retain_graph=True)\n",
    "        d_loss = d_loss_real + d_loss_fake + d_gp_loss\n",
    "        opt_C.step()\n",
    "        d_loss_mean += d_loss.item()\n",
    "    d_loss_mean /= d_iter\n",
    "    D_update_ts = time.time()\n",
    "    # train G:\n",
    "    G_net.train()\n",
    "    C_net.eval() \n",
    "    real, label = next(gen)\n",
    "    real = real.to(device).requires_grad_(True)\n",
    "    label = label.to(device).requires_grad_(True)\n",
    "    opt_G.zero_grad()\n",
    "    generated = G_net(label)\n",
    "    g_loss = C_net(generated,label).mean() + beta * torch.abs(real-generated).mean()\n",
    "    g_loss.backward()\n",
    "    opt_G.step()\n",
    "    g_loss_mean = g_loss.mean().item()\n",
    "    G_update_ts = time.time()\n",
    "    if ite%preview_iter==0:\n",
    "        print('[{}/{}] G: {:.4f}, D:{:.4f} -- elapsed_G: {:.4f}s -- elapsed_D: {:.4f}s'.format(ite, iterations, g_loss_mean, d_loss_mean, (G_update_ts-D_update_ts), (D_update_ts-start_train_ts) ))\n",
    "        label = label[:preview_n].detach().cpu().numpy().transpose(0,2,1)\n",
    "        generated = generated[:preview_n].detach().cpu().numpy().transpose(0,2,1)\n",
    "        label = list(map(lambda x: ohe2str(x, strainset_inv), label))\n",
    "        generated = list(map(lambda x: ohe2str(x, charset_inv), generated))\n",
    "        with open('./previews/iter-{:d}.txt'.format(ite), 'w') as fp:\n",
    "            for l, poet in zip(label,generated):\n",
    "                fp.write(poet[:5]+'，')\n",
    "                fp.write(poet[5:10]+'。')\n",
    "                fp.write(poet[10:15]+'，')\n",
    "                fp.write(poet[15:20]+'。')\n",
    "                fp.write('\\n')\n",
    "                fp.write(l[:5]+'，')\n",
    "                fp.write(l[5:10]+'。')\n",
    "                fp.write(l[10:15]+'，')\n",
    "                fp.write(l[15:20]+'。')\n",
    "                fp.write('\\n')\n",
    "                \n",
    "        \n",
    "        torch.save(G_net.state_dict(), './checkpoints/iter-{:d}-G.ckpt'.format(ite))\n",
    "        torch.save(C_net.state_dict(), './checkpoints/iter-{:d}-D.ckpt'.format(ite))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
